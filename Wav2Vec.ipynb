{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    # Additional steps for deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can replace 42 with any other seed value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset class that returns the raw audio and labels (as tensors) in a single dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "We have 50 training examples and 50 validation examples.\n",
      "The size of our audio embeddings is 465984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "#import wav2vec model and processor\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.processor = processor\n",
    "\n",
    "        # Extract column names for labels dynamically\n",
    "        self.label_columns = list(self.dataframe.columns[:-1])  #Exclude first two columns since these are irrelevant\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.dataframe.iloc[idx]['mp3_path']\n",
    "        \n",
    "        # Select label columns based on the dynamically created list. This is grabbing all 188 class label names and converting to tensors.\n",
    "        labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
    "\n",
    "        # Load raw audio data using librosa\n",
    "        audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        #Use processor to process audio file and return tensor of input values for model\n",
    "        input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        # Return a dictionary with input data and labels\n",
    "        return {'input': input_tensors, 'labels': labels}\n",
    "\n",
    "csv_path_train = 'train_example.csv'\n",
    "csv_path_val = 'valid_example.csv'\n",
    "train_example = CustomAudioDataset(csv_path=csv_path_train, processor=processor)\n",
    "val_example = CustomAudioDataset(csv_path=csv_path_val, processor=processor)\n",
    "embedding_size = train_example[0]['input'].shape[1] #make sure that we can easily modify embedding size input to model\n",
    "\n",
    "print(f'We have {len(train_example)} training examples and {len(val_example)} validation examples.')\n",
    "print(f'The size of our audio embeddings is {embedding_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for training example 0: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 1: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 2: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 3: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 4: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_11886/3265687529.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#print the labels of the first five training examples\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Labels for training example {i}: {train_example[i][\"labels\"]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture. We first use the wav2vec autoencoder to generate audio embeddings. Then we add a few FF layers. Finally, we add a softmax layer for the output since we are doing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just boilerplate code. We can update this if we want to make it deeper etc.\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomAudioModel(nn.Module):\n",
    "    def __init__(self, ff_input_size=embedding_size, ff_output_size=64, num_classes=188):\n",
    "        super(CustomAudioModel, self).__init__()\n",
    "\n",
    "        # Define custom feed-forward layers\n",
    "        self.fc1 = nn.Linear(ff_input_size, ff_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Apply custom feed-forward layers directly to the extracted embeddings\n",
    "        x = self.fc1(embeddings)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training module from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'compute_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Training arguments -- these need to be adjusted\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./results\u001b[39;49m\u001b[39m'\u001b[39;49m,                     \u001b[39m# output directory\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,                         \u001b[39m# total number of training epochs\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,             \u001b[39m# batch size per device during training\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,              \u001b[39m# batch size per device during eval\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m#weight_decay=0.01,                         # regularization parameter\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     logging_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./logs\u001b[39;49m\u001b[39m'\u001b[39;49m,                       \u001b[39m# directory for storing logs\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,                           \u001b[39m# number of steps before logging\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m,                \u001b[39m# evaluate every eval_steps\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,                              \u001b[39m# number of steps before evaluating\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     save_total_limit\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,                         \u001b[39m# limit the total amount of checkpoints. Deletes the older checkpoints.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,                             \u001b[39m# number of updates steps before checkpoint saves\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     compute_loss\u001b[39m=\u001b[39;49mcompute_loss,                  \u001b[39m# custom loss function\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,                        \u001b[39m# defined optimizer\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Trainer instance\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_example,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_example,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'compute_loss'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomAudioModel(ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "\n",
    "# #These need to be updated\n",
    "# csv_path_train = 'data.csv'\n",
    "# csv_path_val = 'data.csv'\n",
    "\n",
    "# #Creating train and validate datasets\n",
    "# train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "# val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "# def compute_loss(model, inputs):\n",
    "#     # Your custom loss calculation goes here\n",
    "#     logits = model(inputs['input'])\n",
    "#     loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "#     return loss\n",
    "\n",
    "#Loss function for binary, multi-label classification\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() #appropiate loss function for multi-label classification where each label is binary. \n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# Training arguments -- these need to be adjusted\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                     # output directory\n",
    "    num_train_epochs=3,                         # total number of training epochs\n",
    "    per_device_train_batch_size=32,             # batch size per device during training\n",
    "    per_device_eval_batch_size=32,              # batch size per device during eval\n",
    "    #weight_decay=0.01,                         # regularization parameter\n",
    "    logging_dir='./logs',                       # directory for storing logs\n",
    "    logging_steps=10,                           # number of steps before logging\n",
    "    evaluation_strategy=\"steps\",                # evaluate every eval_steps\n",
    "    eval_steps=50,                              # number of steps before evaluating\n",
    "    save_total_limit=2,                         # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    save_steps=500,                             # number of updates steps before checkpoint saves\n",
    "    optimizer=optimizer,                        # defined optimizer\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_example,\n",
    "    eval_dataset=val_example,\n",
    "    compute_loss=loss_fn\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "model_path = \"./example_50\"\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"facebook/wav2vec2-base-960h\", output_size=188, ff_output_size=64):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         # Load the Wav2Vec 2.0 model and processor\n",
    "#         self.wav2vec_model = AutoModelForCTC.from_pretrained(wav2vec_model_name)\n",
    "#         self.processor = AutoProcessor.from_pretrained(wav2vec_model_name)\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(768, ff_output_size)  # Adjust input size based on Wav2Vec 2.0 model's hidden size\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, output_size)  # Adjust output size based on your task\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, audio_data):\n",
    "#         # Process audio data using the Wav2Vec 2.0 model\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = self.wav2vec_model(input_tensors).last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Apply custom feed-forward layers\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         output = self.softmax(x)  # Apply sigmoid activation for multi-label classification\n",
    "\n",
    "#         return output\n",
    "\n",
    "#Instantiate the model\n",
    "# model = CustomAudioModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# num_epochs = 3\n",
    "\n",
    "# audio_data = torch.randn(1, 16000)  # Replace with your actual audio data\n",
    "# output = model(audio_data)\n",
    "# print(output.shape)  # This will be (batch_size, output_size), where output_size is 188 in your case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         inputs = batch['input']\n",
    "#         labels = batch['labels']\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "# import torch\n",
    "# import librosa\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset from the CSV file\n",
    "# csv_path = 'path/to/your/csv/file.csv'\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Load pretrained model and processor\n",
    "# model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# class CustomAudioDataset(Dataset):\n",
    "#     def __init__(self, dataframe, processor):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         audio_path = self.dataframe.iloc[idx]['audio_file']\n",
    "#         labels = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "#         # Load audio file and process using the Wav2Vec processor\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "#         return {'input': input_tensors, 'labels': torch.tensor(labels, dtype=torch.float32)}\n",
    "\n",
    "# # Create an instance of your custom dataset\n",
    "# audio_dataset = CustomAudioDataset(df, processor)\n",
    "\n",
    "# # Create a PyTorch DataLoader for batching and shuffling\n",
    "# batch_size = 32  # Adjust as needed\n",
    "# data_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example usage in a training loop\n",
    "# for batch in data_loader:\n",
    "#     inputs = batch['input']\n",
    "#     labels = batch['labels']\n",
    "\n",
    "    # Forward pass, loss calculation, backward pass, optimization, etc.\n",
    "    # Your training code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#iterate over audio file and extract embeddings.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, audio_file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(audio_files):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   embeddings \u001b[39m=\u001b[39m extract_audio_embeddings(audio_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   \u001b[39m#save embeddings in a numpy array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m input_tensors \u001b[39m=\u001b[39m processor(audio_data, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, sampling_rate\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m)\u001b[39m.\u001b[39minput_values \u001b[39m#use processor to process audio file and return tensor of input values for model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   embeddings \u001b[39m=\u001b[39m model(input_tensors)\u001b[39m.\u001b[39;49mlast_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#take the mean of the last hidden state of the model to extract the embeddings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m (embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "# #Path to the directory containing the audio files\n",
    "# train_audio = 'train'\n",
    "\n",
    "# #list all files in the directory\n",
    "# audio_files = [os.path.join(train_audio, file) for file in os.listdir(train_audio)]\n",
    "\n",
    "# #Define number of files to process\n",
    "# num_files = 10\n",
    "\n",
    "# #iterate over audio file and extract embeddings.\n",
    "\n",
    "# for i, audio_file in enumerate(audio_files):\n",
    "#   embeddings = extract_audio_embeddings(audio_file)\n",
    "  \n",
    "#   #save embeddings in a numpy array\n",
    "#   if i == 0:\n",
    "#     embeddings_array = embeddings\n",
    "#   else:\n",
    "#     embeddings_array = np.vstack((embeddings_array, embeddings))\n",
    "    \n",
    "#   #Check if number of files to process has been reached\n",
    "#   if i + 1 == num_files:\n",
    "#     print(f'Processed {num_files} files. Stopping the iteration.')\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
