{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    # Additional steps for deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can replace 42 with any other seed value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset class that returns the raw audio and labels (as tensors) in a single dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "#import wav2vec model and processor\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.processor = processor\n",
    "\n",
    "        # Extract column names for labels dynamically\n",
    "        self.label_columns = list(self.dataframe.columns[2:])  #Exclude first two columns since these are irrelevant\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.dataframe.iloc[idx]['mp3_path']\n",
    "        \n",
    "        # Select label columns based on the dynamically created list. This is grabbing all 188 class label names and converting to tensors.\n",
    "        labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
    "\n",
    "        # Load raw audio data using librosa\n",
    "        audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        #Use processor to process audio file and return tensor of input values for model\n",
    "        input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        # Return a dictionary with input data and labels\n",
    "        return {'input': input_tensors, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture. We first use the wav2vec autoencoder to generate audio embeddings. Then we add a few FF layers. Finally, we add a softmax layer for the output since we are doing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just boilerplate code. We can update this if we want to make it deeper etc.\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomAudioModel(nn.Module):\n",
    "    def __init__(self, ff_input_size=768, ff_output_size=64, num_classes=188):\n",
    "        super(CustomAudioModel, self).__init__()\n",
    "\n",
    "        # Define custom feed-forward layers\n",
    "        self.fc1 = nn.Linear(ff_input_size, ff_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Apply custom feed-forward layers directly to the extracted embeddings\n",
    "        x = self.fc1(embeddings)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.sigmoid(x)  # Apply sigmoid activation for multi-label classification\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training module from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This needs to be updated with the appropriate values\n",
    "model = CustomAudioModel(ff_input_size=768, ff_output_size=64, num_classes=188) #768 is the size of the embeddings from the wav2vec model\n",
    "\n",
    "#These need to be updated\n",
    "csv_path_train = 'data.csv'\n",
    "csv_path_val = 'data.csv'\n",
    "\n",
    "#Creating train and validate datasets\n",
    "train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "def compute_loss(model, inputs):\n",
    "    # Your custom loss calculation goes here\n",
    "    logits = model(inputs['input'])\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "    return loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# Training arguments -- these need to be adjusted\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output_dir',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=1e-4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    compute_loss=compute_loss,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"facebook/wav2vec2-base-960h\", output_size=188, ff_output_size=64):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         # Load the Wav2Vec 2.0 model and processor\n",
    "#         self.wav2vec_model = AutoModelForCTC.from_pretrained(wav2vec_model_name)\n",
    "#         self.processor = AutoProcessor.from_pretrained(wav2vec_model_name)\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(768, ff_output_size)  # Adjust input size based on Wav2Vec 2.0 model's hidden size\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, output_size)  # Adjust output size based on your task\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, audio_data):\n",
    "#         # Process audio data using the Wav2Vec 2.0 model\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = self.wav2vec_model(input_tensors).last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Apply custom feed-forward layers\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         output = self.softmax(x)  # Apply sigmoid activation for multi-label classification\n",
    "\n",
    "#         return output\n",
    "\n",
    "#Instantiate the model\n",
    "# model = CustomAudioModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# num_epochs = 3\n",
    "\n",
    "# audio_data = torch.randn(1, 16000)  # Replace with your actual audio data\n",
    "# output = model(audio_data)\n",
    "# print(output.shape)  # This will be (batch_size, output_size), where output_size is 188 in your case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         inputs = batch['input']\n",
    "#         labels = batch['labels']\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "# import torch\n",
    "# import librosa\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset from the CSV file\n",
    "# csv_path = 'path/to/your/csv/file.csv'\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Load pretrained model and processor\n",
    "# model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# class CustomAudioDataset(Dataset):\n",
    "#     def __init__(self, dataframe, processor):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         audio_path = self.dataframe.iloc[idx]['audio_file']\n",
    "#         labels = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "#         # Load audio file and process using the Wav2Vec processor\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "#         return {'input': input_tensors, 'labels': torch.tensor(labels, dtype=torch.float32)}\n",
    "\n",
    "# # Create an instance of your custom dataset\n",
    "# audio_dataset = CustomAudioDataset(df, processor)\n",
    "\n",
    "# # Create a PyTorch DataLoader for batching and shuffling\n",
    "# batch_size = 32  # Adjust as needed\n",
    "# data_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example usage in a training loop\n",
    "# for batch in data_loader:\n",
    "#     inputs = batch['input']\n",
    "#     labels = batch['labels']\n",
    "\n",
    "    # Forward pass, loss calculation, backward pass, optimization, etc.\n",
    "    # Your training code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#iterate over audio file and extract embeddings.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, audio_file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(audio_files):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   embeddings \u001b[39m=\u001b[39m extract_audio_embeddings(audio_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   \u001b[39m#save embeddings in a numpy array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m input_tensors \u001b[39m=\u001b[39m processor(audio_data, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, sampling_rate\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m)\u001b[39m.\u001b[39minput_values \u001b[39m#use processor to process audio file and return tensor of input values for model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   embeddings \u001b[39m=\u001b[39m model(input_tensors)\u001b[39m.\u001b[39;49mlast_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#take the mean of the last hidden state of the model to extract the embeddings\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m (embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "# #Path to the directory containing the audio files\n",
    "# train_audio = 'train'\n",
    "\n",
    "# #list all files in the directory\n",
    "# audio_files = [os.path.join(train_audio, file) for file in os.listdir(train_audio)]\n",
    "\n",
    "# #Define number of files to process\n",
    "# num_files = 10\n",
    "\n",
    "# #iterate over audio file and extract embeddings.\n",
    "\n",
    "# for i, audio_file in enumerate(audio_files):\n",
    "#   embeddings = extract_audio_embeddings(audio_file)\n",
    "  \n",
    "#   #save embeddings in a numpy array\n",
    "#   if i == 0:\n",
    "#     embeddings_array = embeddings\n",
    "#   else:\n",
    "#     embeddings_array = np.vstack((embeddings_array, embeddings))\n",
    "    \n",
    "#   #Check if number of files to process has been reached\n",
    "#   if i + 1 == num_files:\n",
    "#     print(f'Processed {num_files} files. Stopping the iteration.')\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
