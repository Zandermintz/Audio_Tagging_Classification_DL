{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    # Additional steps for deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can replace 42 with any other seed value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset class that returns the raw audio and labels (as tensors) in a single dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/config.json from cache at /Users/zandermintz/.cache/huggingface/transformers/cbb3014bb9f03ead9b94f4a791ff8e777465307670e85079d35e28cbc5d88727.0e2d739358c9b58747bd19db5f9f4320dacabbeb1e6282f5cc1069c5c55a82d2\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/pytorch_model.bin from cache at /Users/zandermintz/.cache/huggingface/transformers/4cb133d3cf3e58e8a4e088b1fc826611a3bcf3d98b20a0bb49ce8cd5362411b7.beeaccfa4baf44ba6123c23938d8a17f48344361a5e7041782e537dfd78a2037\n",
      "/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading feature extractor configuration file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/preprocessor_config.json from cache at /Users/zandermintz/.cache/huggingface/transformers/07e398f6c4f4eb4f676c75befc5ace223491c79cea1109fb4029751892d380a1.bc3155ca0bae3a39fc37fc6d64829c6a765f46480894658bb21c08db6155358d\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/vocab.json from cache at /Users/zandermintz/.cache/huggingface/transformers/02595a4ae02bcd3f20d5fe783417b9b4ee9d11c244df7e3108bde6c2f37402da.7c838a0a103758bad6ef4922531682da23a8b1c45d25f8d8e7a6d857c0b26544\n",
      "loading file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/tokenizer_config.json from cache at /Users/zandermintz/.cache/huggingface/transformers/a2973721f0d595de6a1c43e48e80dab25bb6c707d364f85a6674c75859942183.b10dd18eae95dde984b32c748781505f0b8c9c20dd067fe083088149f66987c4\n",
      "loading file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/special_tokens_map.json from cache at /Users/zandermintz/.cache/huggingface/transformers/208086b2429fa2ba5b196810c1bcd7d61e2c8d4afd65d05d0670096d735fd5bb.9d6cd81ef646692fb1c169a880161ea1cb95f49694f220aced9b704b457e51dd\n",
      "loading file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/config.json from cache at /Users/zandermintz/.cache/huggingface/transformers/cbb3014bb9f03ead9b94f4a791ff8e777465307670e85079d35e28cbc5d88727.0e2d739358c9b58747bd19db5f9f4320dacabbeb1e6282f5cc1069c5c55a82d2\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-base-960h\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5 training examples and 5 validation examples.\n",
      "The size of our audio embeddings is 465984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "#import wav2vec model and processor\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.processor = processor\n",
    "\n",
    "        # Extract column names for labels dynamically\n",
    "        self.label_columns = list(self.dataframe.columns[:-1])  #Exclude first two columns since these are irrelevant\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.dataframe.iloc[idx]['mp3_path']\n",
    "        \n",
    "        # Select label columns based on the dynamically created list. This is grabbing all 188 class label names and converting to tensors.\n",
    "        labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
    "        \n",
    "        # try:\n",
    "        # Load raw audio data using librosa\n",
    "        audio_data, _ = librosa.load(audio_path, sr=16000, mono=True, res_type=\"kaiser_fast\")\n",
    "        \n",
    "        #Use processor to process audio file and return tensor of input values for model\n",
    "        input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        # except:\n",
    "        #     print(f'Error loading audio file: {audio_path}')\n",
    "        #     raise\n",
    "        # Return a dictionary with input data and labels\n",
    "        return {'embeddings': input_tensors, 'labels': labels}\n",
    "\n",
    "csv_path_train = 'train_example.csv'\n",
    "csv_path_val = 'valid_example.csv'\n",
    "train_example = CustomAudioDataset(csv_path=csv_path_train, processor=processor)\n",
    "val_example = CustomAudioDataset(csv_path=csv_path_val, processor=processor)\n",
    "embedding_size = train_example[0]['embeddings'].shape[1] #make sure that we can easily modify embedding size input to model\n",
    "\n",
    "print(f'We have {len(train_example)} training examples and {len(val_example)} validation examples.')\n",
    "print(f'The size of our audio embeddings is {embedding_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio file.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Replace 'path_to_your_audio_file.wav' with the path to one of your audio files\n",
    "#audio_path = 'train/aba_structure-epic-01-deep_step-291-320.wav'\n",
    "audio_path = 'train/aba_structure-epic-01-deep_step-320-349.wav'\n",
    "\n",
    "try:\n",
    "    audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "    print(\"Successfully loaded audio file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading audio file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for training example 0: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 1: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 2: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 3: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Labels for training example 4: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#print the labels of the first five training examples\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Labels for training example {i}: {train_example[i][\"labels\"]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture. We first use the wav2vec autoencoder to generate audio embeddings. Then we add a few FF layers. Finally, we add a softmax layer for the output since we are doing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is just boilerplate code. We can update this if we want to make it deeper etc.\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomAudioModel(nn.Module):\n",
    "    def __init__(self, dataset, ff_input_size=embedding_size, ff_output_size=64, num_classes=188):\n",
    "        super(CustomAudioModel, self).__init__()\n",
    "        \n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Define custom feed-forward layers\n",
    "        self.fc1 = nn.Linear(ff_input_size*len(self.dataset), ff_output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        # Apply custom feed-forward layers directly to the input_values\n",
    "        #embeddings = torch.cat([batch['embeddings'] for batch in train_example], dim=1) #concatenate the embeddings\n",
    "        embeddings = self.dataset['embeddings']\n",
    "        print(\"Input Shape:\", embeddings.shape)\n",
    "        \n",
    "        embeddings = embeddings.view(embeddings.size(0), -1) #flatten the embeddings\n",
    "        x = self.fc1(embeddings)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Calculate the loss if labels are provided\n",
    "            # Assuming you are using binary cross-entropy loss\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(x, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, dataset, ff_input_size=embedding_size, ff_output_size=64, num_classes=188):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "        \n",
    "#         self.dataset = dataset\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(ff_input_size, ff_output_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "        \n",
    "#         embeddings = self.dataset['embeddings']  # Grab the audio embeddings from the inputs dictionary\n",
    "#         # Flatten the embeddings\n",
    "#         embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "#         # Apply custom feed-forward layers directly to the flattened embeddings\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     from transformers import Trainer, TrainingArguments\n",
    "# from torch.optim import Adam\n",
    "# import torch.nn.functional as F\n",
    "# #%pip install \"transformers[torch]\"\n",
    "\n",
    "# Instantiate the model\n",
    "#model = CustomAudioModel(ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "\n",
    "# # #These need to be updated\n",
    "# # csv_path_train = 'data.csv'\n",
    "# # csv_path_val = 'data.csv'\n",
    "\n",
    "# # #Creating train and validate datasets\n",
    "# # train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "# # val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# # Loss function for multi-label classification\n",
    "# # def compute_loss(model, inputs):\n",
    "# #     # Your custom loss calculation goes here\n",
    "# #     logits = model(inputs['input'])\n",
    "# #     loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "# #     return loss\n",
    "\n",
    "# #Loss function for binary, multi-label classification\n",
    "\n",
    "# #loss_fn = nn.BCEWithLogitsLoss() #appropiate loss function for multi-label classification where each label is binary. \n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# Training arguments -- these need to be adjusted\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',                     # output directory\n",
    "#     num_train_epochs=3,                         # total number of training epochs\n",
    "#     per_device_train_batch_size=32,             # batch size per device during training\n",
    "#     per_device_eval_batch_size=32,              # batch size per device during eval\n",
    "#     #weight_decay=0.01,                         # regularization parameter\n",
    "#     logging_dir='./logs',                       # directory for storing logs\n",
    "#     logging_steps=10,                           # number of steps before logging\n",
    "#     evaluation_strategy=\"steps\",                # evaluate every eval_steps\n",
    "#     eval_steps=50,                              # number of steps before evaluating\n",
    "#     save_total_limit=2,                         # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "#     save_steps=500,                             # number of updates steps before checkpoint saves                       \n",
    "# )\n",
    "\n",
    "# # Trainer instance\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_example,\n",
    "#     eval_dataset=val_example,\n",
    "#     #compute_loss=loss_fn,\n",
    "#     #optimizer=optimizer\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training module from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n",
      "/var/folders/9h/kwql1f1j1bgfmhydqvgpmz500000gn/T/ipykernel_92651/3126874759.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.dataframe.iloc[idx][self.label_columns], dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m#optimizer=optimizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Save the model after training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./example_50\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/transformers/trainer.py:1316\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1319\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1322\u001b[0m ):\n\u001b[1;32m   1323\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/transformers/trainer.py:1849\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   1848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1849\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1852\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/transformers/trainer.py:1881\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1880\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1881\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   1882\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embeddings, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Apply custom feed-forward layers directly to the input_values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m#embeddings = torch.cat([batch['embeddings'] for batch in train_example], dim=1) #concatenate the embeddings\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput Shape:\u001b[39m\u001b[39m\"\u001b[39m, embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mview(embeddings\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#flatten the embeddings\u001b[39;00m\n",
      "\u001b[1;32m/Users/zandermintz/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     audio_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miloc[idx][\u001b[39m'\u001b[39m\u001b[39mmp3_path\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Select label columns based on the dynamically created list. This is grabbing all 188 class label names and converting to tensors.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zandermintz/Desktop/2023-2024%20Academic/Deep%20Learning%20and%20Multimodal%20Data/Audio_Tagging_Classification_DL/Wav2Vec.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miloc[idx][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_columns], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/Desktop/2023-2024 Academic/Deep Learning and Multimodal Data/Audio_Tagging_Classification_DL/.virtual/lib/python3.11/site-packages/pandas/core/indexing.py:1711\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1709\u001b[0m key \u001b[39m=\u001b[39m item_from_zerodim(key)\n\u001b[1;32m   1710\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1711\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index by location index with a non-integer key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1713\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_integer(key, axis)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "#%pip install \"transformers[torch]\"\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomAudioModel(dataset = train_example, ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "\n",
    "# #These need to be updated\n",
    "# csv_path_train = 'data.csv'\n",
    "# csv_path_val = 'data.csv'\n",
    "\n",
    "# #Creating train and validate datasets\n",
    "# train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "# val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# Loss function for multi-label classification\n",
    "# def compute_loss(model, inputs):\n",
    "#     # Your custom loss calculation goes here\n",
    "#     logits = model(inputs['input'])\n",
    "#     loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "#     return loss\n",
    "\n",
    "#Loss function for binary, multi-label classification\n",
    "\n",
    "#loss_fn = nn.BCEWithLogitsLoss() #appropiate loss function for multi-label classification where each label is binary. \n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# Training arguments -- these need to be adjusted\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                     # output directory\n",
    "    num_train_epochs=3,                         # total number of training epochs\n",
    "    per_device_train_batch_size=5,             # batch size per device during training\n",
    "    per_device_eval_batch_size=5,              # batch size per device during eval\n",
    "    #weight_decay=0.01,                         # regularization parameter\n",
    "    logging_dir='./logs',                       # directory for storing logs\n",
    "    logging_steps=10,                           # number of steps before logging\n",
    "    evaluation_strategy=\"steps\",                # evaluate every eval_steps\n",
    "    eval_steps=50,                              # number of steps before evaluating\n",
    "    save_total_limit=2,                         # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    save_steps=500,                             # number of updates steps before checkpoint saves                       \n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_example,\n",
    "    eval_dataset=val_example,\n",
    "    #compute_loss=loss_fn,\n",
    "    #optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "model_path = \"./example_50\"\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"facebook/wav2vec2-base-960h\", output_size=188, ff_output_size=64):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         # Load the Wav2Vec 2.0 model and processor\n",
    "#         self.wav2vec_model = AutoModelForCTC.from_pretrained(wav2vec_model_name)\n",
    "#         self.processor = AutoProcessor.from_pretrained(wav2vec_model_name)\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(768, ff_output_size)  # Adjust input size based on Wav2Vec 2.0 model's hidden size\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, output_size)  # Adjust output size based on your task\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, audio_data):\n",
    "#         # Process audio data using the Wav2Vec 2.0 model\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = self.wav2vec_model(input_tensors).last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Apply custom feed-forward layers\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         output = self.softmax(x)  # Apply sigmoid activation for multi-label classification\n",
    "\n",
    "#         return output\n",
    "\n",
    "#Instantiate the model\n",
    "# model = CustomAudioModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# num_epochs = 3\n",
    "\n",
    "# audio_data = torch.randn(1, 16000)  # Replace with your actual audio data\n",
    "# output = model(audio_data)\n",
    "# print(output.shape)  # This will be (batch_size, output_size), where output_size is 188 in your case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         inputs = batch['input']\n",
    "#         labels = batch['labels']\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "# import torch\n",
    "# import librosa\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset from the CSV file\n",
    "# csv_path = 'path/to/your/csv/file.csv'\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Load pretrained model and processor\n",
    "# model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# class CustomAudioDataset(Dataset):\n",
    "#     def __init__(self, dataframe, processor):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         audio_path = self.dataframe.iloc[idx]['audio_file']\n",
    "#         labels = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "#         # Load audio file and process using the Wav2Vec processor\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "#         return {'input': input_tensors, 'labels': torch.tensor(labels, dtype=torch.float32)}\n",
    "\n",
    "# # Create an instance of your custom dataset\n",
    "# audio_dataset = CustomAudioDataset(df, processor)\n",
    "\n",
    "# # Create a PyTorch DataLoader for batching and shuffling\n",
    "# batch_size = 32  # Adjust as needed\n",
    "# data_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example usage in a training loop\n",
    "# for batch in data_loader:\n",
    "#     inputs = batch['input']\n",
    "#     labels = batch['labels']\n",
    "\n",
    "    # Forward pass, loss calculation, backward pass, optimization, etc.\n",
    "    # Your training code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Path to the directory containing the audio files\n",
    "# train_audio = 'train'\n",
    "\n",
    "# #list all files in the directory\n",
    "# audio_files = [os.path.join(train_audio, file) for file in os.listdir(train_audio)]\n",
    "\n",
    "# #Define number of files to process\n",
    "# num_files = 10\n",
    "\n",
    "# #iterate over audio file and extract embeddings.\n",
    "\n",
    "# for i, audio_file in enumerate(audio_files):\n",
    "#   embeddings = extract_audio_embeddings(audio_file)\n",
    "  \n",
    "#   #save embeddings in a numpy array\n",
    "#   if i == 0:\n",
    "#     embeddings_array = embeddings\n",
    "#   else:\n",
    "#     embeddings_array = np.vstack((embeddings_array, embeddings))\n",
    "    \n",
    "#   #Check if number of files to process has been reached\n",
    "#   if i + 1 == num_files:\n",
    "#     print(f'Processed {num_files} files. Stopping the iteration.')\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
