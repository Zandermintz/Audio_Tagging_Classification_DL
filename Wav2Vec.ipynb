{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
    "s\n",
    "    Args:\n",
    "        seed_value (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    # Additional steps for deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can replace 42 with any other seed value of your choice\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset class that returns audio embeddings (using wav2vec) and labels (as tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "#import wav2vec model and processor\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.processor = processor\n",
    "\n",
    "        # Extract column names for labels dynamically\n",
    "        self.label_columns = list(self.dataframe.columns[:-1])  #Exclude first two columns since these are irrelevant\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the audio path\n",
    "        audio_path = self.dataframe.iloc[idx]['mp3_path']\n",
    "        \n",
    "        # Select label columns based on the dynamically created list. This is grabbing all 188 class label names and converting to tensors.\n",
    "        #labels = self.dataframe.iloc[idx][self.label_columns]\n",
    "        labels_array = self.dataframe.iloc[idx].loc[self.label_columns].astype('float').values\n",
    "        labels = torch.tensor(labels_array, dtype=torch.float32)\n",
    "        #labels = torch.tensor(self.dataframe.iloc[idx].loc[self.label_columns].astype('float').values, dtype=torch.float32)\n",
    "        \n",
    "        # Load raw audio data using librosa\n",
    "        audio_data, _ = librosa.load(audio_path, sr=16000, mono=True, res_type=\"kaiser_fast\")\n",
    "        \n",
    "        #Use processor to process audio file and return tensor of input values for model\n",
    "        input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        return input_tensors, labels\n",
    "    \n",
    "    # Define collate_fn to handle the varying lengths of audio files and labels. This will help with batching.\n",
    "    def collate_fn(self, batch):\n",
    "        input_tensors_batch, labels_batch = zip(*batch)\n",
    "        return torch.stack(input_tensors_batch), torch.stack(labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets for Train/Validate, Instantiating DataLoaders for Batching, Capturing Input_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 50 training examples and 25 validation examples.\n",
      "We have 2 batches in the training set and 1 batches in the validation set.\n",
      "The size of our audio embeddings is 465984.\n"
     ]
    }
   ],
   "source": [
    "#set the correct paths to the csv files\n",
    "csv_path_train = 'train_example.csv'\n",
    "csv_path_val = 'valid_example.csv'\n",
    "csv_path_test = 'test_example.csv'\n",
    "\n",
    "#initialize datasets\n",
    "train_example = CustomAudioDataset(csv_path=csv_path_train, processor=processor)\n",
    "val_example = CustomAudioDataset(csv_path=csv_path_val, processor=processor)\n",
    "test_example = CustomAudioDataset(csv_path=csv_path_test, processor=processor)\n",
    "\n",
    "#set the batch size for all dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "#Dataloader for training\n",
    "train_loader = DataLoader(train_example, batch_size=batch_size, shuffle=True, collate_fn=train_example.collate_fn)\n",
    "\n",
    "#Dataloader for validation\n",
    "val_loader = DataLoader(val_example, batch_size=batch_size, shuffle=False, collate_fn=val_example.collate_fn)\n",
    "\n",
    "#Dataloader for testing\n",
    "test_loader = DataLoader(test_example, batch_size=batch_size, shuffle=False, collate_fn=test_example.collate_fn)\n",
    "\n",
    "#get the size of the audio embeddings\n",
    "input_size = train_example[0][0].shape[1] #make sure that we can easily modify embedding size input to model\n",
    "\n",
    "#Check the size of the datasets and the audio embeddings\n",
    "print(f'We have {len(train_example)} training examples and {len(val_example)} validation examples.')\n",
    "print(f'We have {len(train_loader)} batches in the training set and {len(val_loader)} batches in the validation set.')\n",
    "print(f'The size of our audio embeddings is {input_size}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing batched training data for parameter grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train has 50 audio embeddings.\n",
      "y_train has 50 labels and 188 classes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to gather X_train and y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate over the DataLoader to gather all training data\n",
    "for batch_input, batch_labels in train_loader:\n",
    "    X_train.append(batch_input)\n",
    "    y_train.append(batch_labels)\n",
    "\n",
    "# Concatenate the lists to create tensors\n",
    "X_train = torch.cat(X_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays\n",
    "X_train_numpy = X_train.numpy()\n",
    "y_train_numpy = y_train.numpy()\n",
    "\n",
    "#check length of X_train and y_train\n",
    "\n",
    "print(f'X_train has {len(X_train_numpy)} audio embeddings.')\n",
    "print(f'y_train has {len(y_train_numpy)} labels and {y_train_numpy.shape[1]} classes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture: Custom FFNN with two layers, ReLU activation, Dropout, and Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update if we want to make it deeper etc.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, num_classes, dropout_rate=0.5):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(input_size, output_size)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(output_size, num_classes)\n",
    "#         #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#     # Apply custom feed-forward layers directly to the input_values\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         #x = self.softmax(x)\n",
    "        \n",
    "#         #remove extra dimension\n",
    "#         x = x.squeeze(1)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "class CustomAudioModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate=0.5):\n",
    "        super(CustomAudioModel, self).__init__()\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Define pooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the size of the flattened output after convolution and pooling\n",
    "        conv_output_size = input_size // 8 * 128\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension to the input\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Apply convolutional and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten the output for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance in Train, Val, Test data by passing new weights into Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 188 labels in our training set. The updated weights for the minority label ('1') within each column are:  tensor([[15.6667,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 49.0000, 15.6667,  1.0000,\n",
      "          1.0000,  1.0000,  6.1429,  1.0000,  1.0000,  1.0000, 49.0000, 49.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, 49.0000, 49.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 49.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, 49.0000,  7.3333,  9.0000,  2.8462,\n",
      "          1.0000, 49.0000,  1.0000,  1.0000, 49.0000, 15.6667,  1.0000,  1.0000,\n",
      "         49.0000,  1.0000,  1.0000,  1.0000, 15.6667,  1.0000,  1.0000, 49.0000,\n",
      "          1.0000, 49.0000,  1.0000,  1.0000,  1.0000,  1.0000,  4.5556,  1.5000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 11.5000,  1.0000,\n",
      "          1.0000, 15.6667,  1.0000,  1.0000,  5.2500,  1.0000,  1.0000, 49.0000,\n",
      "         49.0000,  1.0000,  1.0000,  1.0000,  7.3333,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, 15.6667,  1.0000, 15.6667,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000, 24.0000, 49.0000,  1.0000,  1.0000,  1.0000, 24.0000,  1.0000,\n",
      "         49.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000, 15.6667,  1.0000, 49.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000, 49.0000, 49.0000,  1.0000, 49.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 49.0000, 15.6667,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000, 49.0000,  1.0000,  1.0000, 24.0000,\n",
      "          1.0000, 49.0000, 24.0000,  1.0000,  1.0000, 49.0000, 15.6667,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.7778,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  2.5714, 49.0000, 15.6667,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 24.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000]])\n",
      "We have 188 labels in our validation set. The updated weights for the minority label ('1') within each column are:  tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  2.5714,  1.0000,  1.0000,  1.0000,  3.1667,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 24.0000,\n",
      "          3.1667,  1.0000,  1.0000,  1.0000,  3.1667,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, 24.0000, 24.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, 11.5000,  1.0000,  1.0000, 24.0000, 24.0000, 24.0000,\n",
      "          1.0000,  1.0000, 11.5000,  1.0000,  1.0000, 11.5000,  1.0000, 24.0000,\n",
      "          1.0000,  1.0000,  1.0000, 24.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         11.5000,  1.0000,  1.0000,  1.0000,  1.0000, 24.0000,  3.1667,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 11.5000, 24.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, 24.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 24.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, 24.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          5.2500,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000, 24.0000,\n",
      "          1.0000,  1.0000,  4.0000,  0.7857,  1.0000,  1.0000, 24.0000,  4.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  4.0000,\n",
      "          1.0000,  1.0000,  7.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  7.3333,  7.3333,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000, 24.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000]])\n",
      "We have 188 labels in our test set. The updated weights for the minority label ('1') within each column are:  tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 4.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 4.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 4.0000, 1.0000, 1.0000, 1.0000, 4.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2500, 1.0000,\n",
      "         1.0000, 4.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Function to generate class weights to be applied to the minority class ('1') in each label column to address class imbalance.\n",
    "\n",
    "def class_weights(data_df):\n",
    "    class_weights = []\n",
    "    total_samples = len(data_df)\n",
    "    \n",
    "    for col in data_df.columns:\n",
    "        class_counts = data_df[col].sum()\n",
    "        if class_counts == 0:\n",
    "            imbalance_ratio = 1.0 #if there are no positive samples, set imbalance ratio to 1\n",
    "        else:\n",
    "            imbalance_ratio = (total_samples - class_counts) / class_counts #calculate imbalance ratio. This is the ratio of negative to positive samples.\n",
    "        class_weights.append(torch.tensor([1.0, imbalance_ratio], dtype=torch.float32)) #append to list of class weights\n",
    "    class_weights_tensor = torch.tensor([weight[1] for weight in class_weights], dtype=torch.float32).unsqueeze(0) #extract the second value of each tensor in the list since this is the minority class weight. Then convert to tensor.\n",
    "    return class_weights_tensor\n",
    "\n",
    "#In training data, generate weights specific to label column distribution to apply to each of the 188 labels to account for class imbalance.\n",
    "train_df = pd.read_csv('train_example.csv')\n",
    "train_df = train_df.drop(['mp3_path'], axis=1)\n",
    "train_weights_balanced = class_weights(train_df)\n",
    "print(f\"We have {(train_weights_balanced.shape[1])} labels in our training set. The updated weights for the minority label ('1') within each column are: \", train_weights_balanced)\n",
    "\n",
    "#In validation data, generate weights specific to label column distribution to apply to each of the 188 labels to account for class imbalance.\n",
    "val_df = pd.read_csv('valid_example.csv')\n",
    "val_df = val_df.drop(['mp3_path'], axis=1)\n",
    "val_weights_balanced = class_weights(val_df)\n",
    "print(f\"We have {(val_weights_balanced.shape[1])} labels in our validation set. The updated weights for the minority label ('1') within each column are: \", val_weights_balanced)\n",
    "\n",
    "#In test data, generate weights specific to label column distribution to apply to each of the 188 labels to account for class imbalance.\n",
    "test_df = pd.read_csv('test_example.csv')\n",
    "test_df = test_df.drop(['mp3_path'], axis=1)\n",
    "test_weights_balanced = class_weights(test_df)\n",
    "print(f\"We have {(test_weights_balanced.shape[1])} labels in our test set. The updated weights for the minority label ('1') within each column are: \", test_weights_balanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model, specify hyperparameters, train the model and print the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install skorch\n",
    "# # import skorch\n",
    "# # from skorch import NeuralNetClassifier\n",
    "# # from sklearn.model_selection import RandomizedSearchCV\n",
    "# # from scipy.stats import loguniform\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# #define hyperparameters grid or distribution\n",
    "\n",
    "# num_classes = 188 #number of classes is 188\n",
    "# weight_decay = .01 #weight decay is .01\n",
    "# dropout_rate= 0.5 #dropout rate is 0.5\n",
    "# hidden_size= 512 #hidden size is 512\n",
    "# output_size= hidden_size//2 #output size is half of hidden size\n",
    "# criterion_train = nn.BCEWithLogitsLoss(pos_weight=train_weights_balanced)  #use BCEWithLogitsLoss with class weights for training\n",
    "# # optimizer = optim.Adam(model.parameters(), lr=.01, weight_decay=weight_decay) #use Adam optimizer with learning rate of .01, and L2 regularization with weight decay of 1e-5\n",
    "# num_epochs = 5 \n",
    "\n",
    "# param_grid = dict(hidden_size = [256,512])\n",
    "\n",
    "# def create_model(lr, hidden_size, dropout_rate):\n",
    "#     model = CustomAudioModel(\n",
    "#         input_size=input_size,\n",
    "#         hidden_size=hidden_size,\n",
    "#         output_size=output_size,\n",
    "#         num_classes=num_classes,\n",
    "#         dropout_rate=dropout_rate\n",
    "#     )\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     return model, optimizer\n",
    "\n",
    "# model = CustomAudioModel(\n",
    "#     input_size=input_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     output_size=output_size,\n",
    "#     num_classes=num_classes,\n",
    "#     dropout_rate=dropout_rate\n",
    "# )\n",
    "\n",
    "\n",
    "# # #initialize model\n",
    "# # model = CustomAudioModel(input_size, hidden_size=hidden_size, output_size=output_size, num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "\n",
    "# # grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "# # grid_result = grid.fit(X_train_numpy, y_train_numpy)\n",
    "# # skorch_model = NeuralNetClassifier(\n",
    "# #     CustomAudioModel,\n",
    "# #     criterion=criterion_train,\n",
    "# #     max_epochs=num_epochs,\n",
    "# #     module__input_size=input_size,\n",
    "# #     module__hidden_size=hidden_size,\n",
    "# #     module__output_size=output_size,\n",
    "# #     module__num_classes=num_classes,\n",
    "# #     module__dropout_rate=dropout_rate,\n",
    "# #     optimizer=optimizer,\n",
    "# # )\n",
    "\n",
    "# # param_dist = {\n",
    "# #     'lr': loguniform(1e-4, 1e-1),\n",
    "# #     'module__hidden_size': [256, 512],\n",
    "# #     'module__dropout_rate': [0.3, 0.5, 0.7],\n",
    "# # }\n",
    "\n",
    "# # random_search = RandomizedSearchCV(\n",
    "# #     skorch_model,\n",
    "# #     param_distributions=param_dist,\n",
    "# #     n_iter=10,\n",
    "# #     scoring='average_precision',\n",
    "# #     cv=3,\n",
    "# #     n_jobs=-1,\n",
    "# # )\n",
    "\n",
    "# # # Fit the RandomizedSearchCV\n",
    "# # random_search.fit(X_train_numpy, y_train_numpy)  # Provide your training data here\n",
    "\n",
    "# # # Print the best parameters and corresponding score\n",
    "# # print(\"Best Parameters: \", random_search.best_params_)\n",
    "# # print(\"Best Score: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wrap the PyTorch model in a scikit-learn estimator\n",
    "# from sklearn.base import is_classifier\n",
    "# from sklearn.utils import check_X_y\n",
    "# from sklearn.utils.validation import check_is_fitted\n",
    "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# class PyTorchEstimator(BaseEstimator, ClassifierMixin):\n",
    "#     def __init__(self, model, criterion, optimizer, num_epochs):\n",
    "#         self.model = model\n",
    "#         self.criterion = criterion\n",
    "#         self.optimizer = optimizer\n",
    "#         self.num_epochs = num_epochs\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # Convert input data to torch.Tensor if not already\n",
    "#         X, y = check_X_y(X, y, device='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float32)\n",
    "        \n",
    "#         self.model, self.optimizer = create_model(\n",
    "#             self.input_size, self.hidden_size, self.output_size, self.num_classes, self.dropout_rate\n",
    "#         )\n",
    "#         # Train the PyTorch model\n",
    "#         for epoch in range(self.num_epochs):\n",
    "#             # Your training loop here\n",
    "#             pass\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         # Implement prediction logic\n",
    "#         pass\n",
    "\n",
    "# # Create the PyTorchEstimator\n",
    "# pytorch_estimator = PyTorchEstimator(model, criterion_train, optim.Adam(model.parameters()), num_epochs)\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(pytorch_estimator, param_grid, scoring='average_precision', cv=3, n_jobs=-1)\n",
    "\n",
    "# # Fit the GridSearchCV\n",
    "# grid_search.fit(X_train_numpy, y_train_numpy)\n",
    "\n",
    "# # Print the best parameters and corresponding score\n",
    "# print(\"Best Parameters: \", grid_search.best_params_)\n",
    "# print(\"Best Score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Layer: conv1.weight, Gradient mean: 4.923749656882137e-06, Gradient std: 0.00015924274339340627\n",
      "Epoch 1, Layer: conv1.bias, Gradient mean: 4.543730756267905e-05, Gradient std: 0.00016900950868148357\n",
      "Epoch 1, Layer: conv2.weight, Gradient mean: 3.380043926881626e-05, Gradient std: 0.00014743709471076727\n",
      "Epoch 1, Layer: conv2.bias, Gradient mean: 8.952962525654584e-05, Gradient std: 0.00029948592418804765\n",
      "Epoch 1, Layer: conv3.weight, Gradient mean: 2.2013147827237844e-05, Gradient std: 0.00012704973050858825\n",
      "Epoch 1, Layer: conv3.bias, Gradient mean: 0.00017347076209262013, Gradient std: 0.0006632576696574688\n",
      "Epoch 1, Layer: fc1.weight, Gradient mean: 3.6731373711518245e-06, Gradient std: 6.22087245574221e-05\n",
      "Epoch 1, Layer: fc1.bias, Gradient mean: 8.619744767202064e-05, Gradient std: 0.00081253657117486\n",
      "Epoch 1, Layer: fc2.weight, Gradient mean: 3.381079659448005e-05, Gradient std: 9.455285908188671e-05\n",
      "Epoch 1, Layer: fc2.bias, Gradient mean: 0.0019013748969882727, Gradient std: 0.0014467736473307014\n",
      "Epoch 1/5, Loss: 0.8398625254631042\n",
      "Epoch 2, Layer: conv1.weight, Gradient mean: 4.975543197360821e-05, Gradient std: 0.0004137555952183902\n",
      "Epoch 2, Layer: conv1.bias, Gradient mean: 0.00012952694669365883, Gradient std: 0.0003617584879975766\n",
      "Epoch 2, Layer: conv2.weight, Gradient mean: 8.408344001509249e-05, Gradient std: 0.0003463812463451177\n",
      "Epoch 2, Layer: conv2.bias, Gradient mean: 0.00028614921029657125, Gradient std: 0.0007167170406319201\n",
      "Epoch 2, Layer: conv3.weight, Gradient mean: 4.560363959171809e-05, Gradient std: 0.00028460752218961716\n",
      "Epoch 2, Layer: conv3.bias, Gradient mean: 0.0003790334740187973, Gradient std: 0.0015469396021217108\n",
      "Epoch 2, Layer: fc1.weight, Gradient mean: 9.292293725593481e-06, Gradient std: 0.00013965475955046713\n",
      "Epoch 2, Layer: fc1.bias, Gradient mean: 0.0002291928103659302, Gradient std: 0.0019173239124938846\n",
      "Epoch 2, Layer: fc2.weight, Gradient mean: 9.765133290784433e-05, Gradient std: 0.00020664242038037628\n",
      "Epoch 2, Layer: fc2.bias, Gradient mean: 0.00565095990896225, Gradient std: 0.003926473204046488\n",
      "Epoch 2/5, Loss: 0.7886561751365662\n",
      "Epoch 3, Layer: conv1.weight, Gradient mean: 0.00013638923701364547, Gradient std: 0.0006894423859193921\n",
      "Epoch 3, Layer: conv1.bias, Gradient mean: 0.00018108323274645954, Gradient std: 0.0005638180882669985\n",
      "Epoch 3, Layer: conv2.weight, Gradient mean: 0.00012795180373359472, Gradient std: 0.0005341138457879424\n",
      "Epoch 3, Layer: conv2.bias, Gradient mean: 0.0004021923232357949, Gradient std: 0.0011557560646906495\n",
      "Epoch 3, Layer: conv3.weight, Gradient mean: 7.108671707101166e-05, Gradient std: 0.0004342035681474954\n",
      "Epoch 3, Layer: conv3.bias, Gradient mean: 0.0006049660150893033, Gradient std: 0.002372485352680087\n",
      "Epoch 3, Layer: fc1.weight, Gradient mean: 1.6021083865780383e-05, Gradient std: 0.000210547266760841\n",
      "Epoch 3, Layer: fc1.bias, Gradient mean: 0.00039634195854887366, Gradient std: 0.002958140568807721\n",
      "Epoch 3, Layer: fc2.weight, Gradient mean: 0.00017070856119971722, Gradient std: 0.0003034026885870844\n",
      "Epoch 3, Layer: fc2.bias, Gradient mean: 0.00983221922069788, Gradient std: 0.00575738400220871\n",
      "Epoch 3/5, Loss: 0.8599594235420227\n",
      "Epoch 4, Layer: conv1.weight, Gradient mean: 0.00012082787725375965, Gradient std: 0.0009380230912938714\n",
      "Epoch 4, Layer: conv1.bias, Gradient mean: 0.0002703035424929112, Gradient std: 0.0008419587393291295\n",
      "Epoch 4, Layer: conv2.weight, Gradient mean: 0.00016513356240466237, Gradient std: 0.0007160305394791067\n",
      "Epoch 4, Layer: conv2.bias, Gradient mean: 0.0005143248708918691, Gradient std: 0.0015875461976975203\n",
      "Epoch 4, Layer: conv3.weight, Gradient mean: 0.00010826721700141206, Gradient std: 0.000612480565905571\n",
      "Epoch 4, Layer: conv3.bias, Gradient mean: 0.0008805884863249958, Gradient std: 0.0033351383171975613\n",
      "Epoch 4, Layer: fc1.weight, Gradient mean: 2.1877265680814162e-05, Gradient std: 0.0002884540590457618\n",
      "Epoch 4, Layer: fc1.bias, Gradient mean: 0.0005448737065307796, Gradient std: 0.004093247465789318\n",
      "Epoch 4, Layer: fc2.weight, Gradient mean: 0.00024398915411438793, Gradient std: 0.00040890133823268116\n",
      "Epoch 4, Layer: fc2.bias, Gradient mean: 0.013981334865093231, Gradient std: 0.007703835144639015\n",
      "Epoch 4/5, Loss: 0.9851876497268677\n",
      "Epoch 5, Layer: conv1.weight, Gradient mean: 0.0002048555325018242, Gradient std: 0.00116628035902977\n",
      "Epoch 5, Layer: conv1.bias, Gradient mean: 0.00030835531651973724, Gradient std: 0.0009646276012063026\n",
      "Epoch 5, Layer: conv2.weight, Gradient mean: 0.00020712563127744943, Gradient std: 0.000942314974963665\n",
      "Epoch 5, Layer: conv2.bias, Gradient mean: 0.0006027670460753143, Gradient std: 0.0020643712487071753\n",
      "Epoch 5, Layer: conv3.weight, Gradient mean: 0.00013411980762612075, Gradient std: 0.0007807221845723689\n",
      "Epoch 5, Layer: conv3.bias, Gradient mean: 0.0009982932824641466, Gradient std: 0.004183498676866293\n",
      "Epoch 5, Layer: fc1.weight, Gradient mean: 2.526780190237332e-05, Gradient std: 0.0003711349272634834\n",
      "Epoch 5, Layer: fc1.bias, Gradient mean: 0.0006331579643301666, Gradient std: 0.005176816135644913\n",
      "Epoch 5, Layer: fc2.weight, Gradient mean: 0.0003032677050214261, Gradient std: 0.0005399015499278903\n",
      "Epoch 5, Layer: fc2.bias, Gradient mean: 0.01731174997985363, Gradient std: 0.010914023965597153\n",
      "Epoch 5/5, Loss: 0.8489952087402344\n"
     ]
    }
   ],
   "source": [
    "# hidden_size=512\n",
    "# output_size=hidden_size//2 #this is the size of the hidden layer output from first feed-forward layer\n",
    "num_classes=188 #this is the number of labels we have\n",
    "dropout_rate=0.7 #this is the dropout rate for the dropout layer\n",
    "criterion_train = nn.BCEWithLogitsLoss(pos_weight=train_weights_balanced) #use this loss function since we are doing multilabel classification. pos_weight is the weight to apply to the minority class ('1') in each label column to address class imbalance.\n",
    "weight_decay = .01 #this is the L2 regularization weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=.01, weight_decay=weight_decay) #use Adam optimizer with learning rate of .01, and L2 regularization with weight decay of 1e-5\n",
    "num_epochs = 5 \n",
    "\n",
    "#initialize model\n",
    "model = CustomAudioModel(input_size,num_classes=num_classes, dropout_rate=dropout_rate)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        embeddings, labels = batch\n",
    "        #embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        embeddings = embeddings.squeeze(1)\n",
    "        outputs = model(embeddings)\n",
    "        # print(\"Input shape:\", embeddings.shape)\n",
    "        # print(\"Labels shape:\", labels.shape)\n",
    "        # print(\"Output shape:\", outputs.shape)\n",
    "        loss = criterion_train(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Print gradients only for the first batch in each epoch\n",
    "        if batch_idx == 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"Epoch {epoch+1}, Layer: {name}, Gradient mean: {param.grad.mean().item()}, Gradient std: {param.grad.std().item()}\")\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/nxnvhg5d02q780sbwrdh45140000gn/T/ipykernel_56381/1880604073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mval_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "criterion_val = nn.BCEWithLogitsLoss(pos_weight=val_weights_balanced) #use this loss function since we are doing multilabel classification. pos_weight is the weight to apply to the minority class ('1') in each label column to address class imbalance.\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for val_batch in val_loader:\n",
    "#             embeddings_val, labels_val = val_batch\n",
    "#             embeddings_val = embeddings_val.squeeze(1)\n",
    "#             # Forward pass (no optimization in validation)\n",
    "#             val_outputs = model(embeddings_val)\n",
    "\n",
    "#             # Compute validation loss\n",
    "#             loss = criterion_val(val_outputs, labels_val)\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             #Convert probabilities to binary predictions using a threshold of 0.5\n",
    "#             predictions = (torch.sigmoid(val_outputs) > 0.5).float()\n",
    "            \n",
    "#             #Collect predictions and true labels for precision, recall and F1 score\n",
    "#             val_pred.append(predictions.cpu().numpy())\n",
    "#             val_true.append(labels_val.cpu().numpy())\n",
    "            \n",
    "#             #Compute validation accuracy\n",
    "#             correct_predictions += (predictions == labels_val).sum().item()\n",
    "#             total_samples += labels_val.numel()\n",
    "#             # Print validation loss after each epoch\n",
    "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "# # Calculate average validation loss\n",
    "# # avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "# #Concatenate predictions and true labels along the sample axis\n",
    "# val_pred = np.concatenate(val_pred, axis=0)\n",
    "# val_true = np.concatenate(val_true, axis=0)\n",
    "\n",
    "# #Calculate overall validation accuracy\n",
    "# val_accuracy = round(((correct_predictions / total_samples)*100), 1)\n",
    "\n",
    "# #Compute precision, recall and F1 score\n",
    "# precision = precision_score(val_true, val_pred, average='micro') #micro is used to account for class imbalance\n",
    "# recall = recall_score(val_true, val_pred, average='micro') #micro is used to account for class imbalance\n",
    "# f1 = f1_score(val_true, val_pred, average='micro') #micro is used to account for class imbalance\n",
    "\n",
    "# #compute confusion matrix\n",
    "\n",
    "# conf_matrix_val = confusion_matrix(val_true.flatten(), val_pred.flatten())\n",
    "\n",
    "# # Print training and validation metrics after each epoch\n",
    "# print(f\"Overall Validation Accuracy: {val_accuracy}%, Validation Precision: {precision}, Validation Recall: {recall}, Validation F1: {f1}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix_val)\n",
    "# print(f'The model has a true negative count of: {conf_matrix_val[0,0]}')\n",
    "# print(f'The model has a false positive count of: {conf_matrix_val[0,1]}')\n",
    "# print(f'The model has a true positive count of: {conf_matrix_val[1,1]}')\n",
    "# print(f'The model has a false negative count of: {conf_matrix_val[1,0]}')\n",
    "\n",
    "val_pred = []\n",
    "val_true = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            embeddings_val, labels_val = val_batch\n",
    "            embeddings_val = embeddings_val.squeeze(1)\n",
    "            # Forward pass (no optimization in validation)\n",
    "            val_outputs = model(embeddings_val)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = criterion_val(val_outputs, labels_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "            predictions = (torch.sigmoid(val_outputs) > 0.5).float()\n",
    "\n",
    "            # Collect predictions and true labels for precision, recall, and F1 score\n",
    "#             val_pred.append(predictions)\n",
    "#             val_true.append(labels_val)\n",
    "            np.append(val_pred, predictions)\n",
    "            np.append(val_true, labels_val)\n",
    "\n",
    "            # Compute validation accuracy\n",
    "            correct_predictions += (predictions == labels_val).sum().item()\n",
    "            total_samples += labels_val.numel()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Concatenate predictions and true labels along the sample axis\n",
    "        print(val_pred)\n",
    "        print(val_true)\n",
    "        val_pred = np.concatenate(val_pred, axis=0)\n",
    "        val_true = np.concatenate(val_true, axis=0)\n",
    "\n",
    "        # Calculate overall validation accuracy\n",
    "        # Note: Accuracy is not a common metric for multilabel classification, consider using precision, recall, and F1 score.\n",
    "        \n",
    "        # Compute precision, recall, and F1 score\n",
    "        precision = precision_score(val_true, val_pred, average='micro')\n",
    "        recall = recall_score(val_true, val_pred, average='micro')\n",
    "        f1 = f1_score(val_true, val_pred, average='micro')\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix_val = confusion_matrix(val_true.flatten(), val_pred.flatten())\n",
    "\n",
    "        # Print training and validation metrics after each epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "        print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1: {f1}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix_val)\n",
    "        print(f'The model has a true negative count of: {conf_matrix_val[0,0]}')\n",
    "        print(f'The model has a false positive count of: {conf_matrix_val[0,1]}')\n",
    "        print(f'The model has a true positive count of: {conf_matrix_val[1,1]}')\n",
    "        print(f'The model has a false negative count of: {conf_matrix_val[1,0]}')\n",
    "\n",
    "        \n",
    "#Save model\n",
    "model_path = \"./model_50N\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "criterion_test = nn.BCEWithLogitsLoss(pos_weight=test_weights_balanced) #use this loss function since we are doing multilabel classification. pos_weight is the weight to apply to the minority class ('1') in each label column to address class imbalance.\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for epoch in range(num_epochs):\n",
    "        test_loss = 0.0 #initialize validation loss\n",
    "        for test_batch in test_loader:\n",
    "            embeddings_test, labels_test = test_batch\n",
    "            \n",
    "            # Forward pass (no optimization in validation)\n",
    "            test_outputs = model(embeddings_test)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = criterion_test(test_outputs, labels_test)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            #Convert probabilities to binary predictions using a threshold of 0.5\n",
    "            predictions = (torch.sigmoid(test_outputs) > 0.5).float()\n",
    "            \n",
    "            #Collect predictions and true labels for precision, recall and F1 score\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "            y_true.append(labels_test.cpu().numpy())\n",
    "            \n",
    "            #Compute validation accuracy\n",
    "            correct_predictions += (predictions == labels_test).sum().item()\n",
    "            total_samples += labels_test.numel()\n",
    "            #print test loss after each epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Test Loss: {loss.item()}\")\n",
    "\n",
    "# Calculate average validation loss\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "#Concatenate predictions and true labels along the sample axis\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "#Calculate overall validation accuracy\n",
    "test_accuracy = round(((correct_predictions / total_samples)*100), 1)\n",
    "\n",
    "#Compute precision, recall and F1 score\n",
    "precision = precision_score(y_true, y_pred, average='micro') #micro is used to account for class imbalance\n",
    "recall = recall_score(y_true, y_pred, average='micro') #micro is used to account for class imbalance\n",
    "f1 = f1_score(y_true, y_pred, average='micro') #micro is used to account for class imbalance\n",
    "\n",
    "#compute confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_test)\n",
    "print(f'The model has a true negative count of: {conf_matrix_test[0,0]}')\n",
    "print(f'The model has a false positive count of: {conf_matrix_test[0,1]}')\n",
    "print(f'The model has a true postive count of: {conf_matrix_test[1,1]}')\n",
    "print(f'The model has a false negative count of: {conf_matrix_test[1,0]}')\n",
    "\n",
    "# Print training and validation metrics after each epoch\n",
    "print(f\"Overall Test Accuracy: {test_accuracy}%, Test Precision: {precision}, Test Recall: {recall}, Test F1: {f1}\")\n",
    "\n",
    "#Save model\n",
    "model_path = \"./final_model\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_path_test = 'test_example.csv'\n",
    "# test_example = CustomAudioDataset(csv_path=csv_path_test, processor=processor)\n",
    "# test_loader = DataLoader(test_example, batch_size=8, shuffle=False, collate_fn=test_example.collate_fn)\n",
    "\n",
    "# #load model\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "\n",
    "# #initialize lists to store labels and predictions\n",
    "\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         embeddings_test, labels_test = batch\n",
    "\n",
    "#         # Forward pass (no optimization in validation)\n",
    "#         test_outputs = model(embeddings_test)\n",
    "        \n",
    "#         # Compute test loss\n",
    "#         test_loss = criterion(test_outputs, labels_test)\n",
    "\n",
    "#         # Convert predictions to binary (0 or 1)\n",
    "#         predicted_labels = (test_outputs > 0.5).float()\n",
    "\n",
    "#         # Print predicted labels\n",
    "#         all_predictions.append(predicted_labels.numpy())\n",
    "#         print(\"Predicted Labels:\")\n",
    "#         print(predicted_labels.numpy())  \n",
    "\n",
    "#         # Print true labels\n",
    "#         all_labels.append(labels_test.numpy())\n",
    "#         print(\"True Labels:\")\n",
    "#         print(labels_test.numpy())  \n",
    "\n",
    "#         #get predictions\n",
    "#         preds = torch.round(test_outputs)\n",
    "        \n",
    "#         #store predictions and labels for later use\n",
    "#         y_true.extend(labels_test)\n",
    "#         y_pred.extend(preds)\n",
    "\n",
    "# #print loss and accuracy\n",
    "\n",
    "# print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as Adam\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = AudioTaggingModel(input_size, 188)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = Adam.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# num_epochs = 5 \n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for embeddings, labels in train_example:\n",
    "# #         embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "#         outputs = model(embeddings).flatten()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# val_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for embeddings_val, labels_val in val_example:\n",
    "# #         embeddings_val, labels_val = embeddings_val.to(device), labels_val.to(device)\n",
    "\n",
    "#         # Forward pass (no optimization in validation)\n",
    "#         outputs_val = model(embeddings_val).flatten()\n",
    "\n",
    "#         # Compute validation loss\n",
    "#         loss_val = criterion(outputs_val, labels_val)\n",
    "#         val_loss += loss_val.item()\n",
    "\n",
    "# model_path = \"./model_path\"\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "# # Calculate average validation loss\n",
    "# avg_val_loss = val_loss / len(val_example)\n",
    "\n",
    "# # Print training and validation loss after each epoch\n",
    "# print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Test Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "# csv_path_test = 'test_example.csv'\n",
    "# test_example = CustomAudioDataset(csv_path=csv_path_test, processor=processor)\n",
    "# # Assuming you have test_loader defined\n",
    "\n",
    "\n",
    "# # Assuming you have already trained and saved your model\n",
    "# # If not, load your pre-trained model here\n",
    "# model = AudioTaggingModel(465984, 188)\n",
    "# model_path = \"./model_path\"\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()\n",
    "\n",
    "# # Define your criterion (loss function)\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Test\n",
    "# test_loss = 0.0\n",
    "# all_predictions = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings_test, labels_test in test_example:\n",
    "# #         inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs_test = model(embeddings_test).flatten()\n",
    "\n",
    "#         # Compute test loss\n",
    "#         loss_test = criterion(outputs_test, labels_test)\n",
    "#         test_loss += loss_test.item()\n",
    "\n",
    "#         # Convert predictions to binary (0 or 1)\n",
    "#         predicted_labels = (outputs_test > 0.5).float()\n",
    "\n",
    "#         # Print predicted labels\n",
    "#         all_predictions.append(predicted_labels.numpy())\n",
    "#         print(\"Predicted Labels:\")\n",
    "#         print(predicted_labels.numpy())  \n",
    "\n",
    "#         # Print true labels\n",
    "#         all_labels.append(labels_test.numpy())\n",
    "#         print(\"True Labels:\")\n",
    "#         print(labels_test.numpy())  \n",
    "\n",
    "# # Calculate average test loss\n",
    "# avg_test_loss = test_loss / len(test_example)\n",
    "\n",
    "# print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# all_predictions = np.array(all_predictions)\n",
    "# all_labels = np.array(all_labels)\n",
    "\n",
    "# # Convert probabilities to binary predictions\n",
    "# binary_preds = np.argmax(all_preds, axis=1)\n",
    "# binary_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# # Print classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(binary_labels, binary_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, ff_output_size, dataset, ff_input_size=input_size):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "        \n",
    "#         self.dataset = dataset\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(ff_input_size*len(self.dataset), ff_output_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "#         #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, embeddings, labels=None):\n",
    "#         # Apply custom feed-forward layers directly to the input_values\n",
    "#         #embeddings = torch.cat([batch['embeddings'] for batch in train_example], dim=1) #concatenate the embeddings\n",
    "# #         embeddings = self.dataset['embeddings']\n",
    "#         print(\"Input Shape:\", embeddings.shape)\n",
    "        \n",
    "#         embeddings = embeddings.view(embeddings.size(0), -1) #flatten the embeddings\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         if labels is not None:\n",
    "#             # Calculate the loss if labels are provided\n",
    "#             # Assuming you are using binary cross-entropy loss\n",
    "#             loss_fn = nn.BCEWithLogitsLoss()\n",
    "#             loss = loss_fn(x, labels)\n",
    "#             return loss\n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class AudioTaggingModel(nn.Module):\n",
    "#     def __init__(self, ff_embedding_size, ff_output_size):\n",
    "#         super(AudioTaggingModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(ff_embedding_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, ff_output_size)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self, embeddings):\n",
    "#         embeddings = self.fc1(embeddings)\n",
    "#         embeddings = self.relu(embeddings)\n",
    "#         embeddings = self.fc2(embeddings)\n",
    "#         embeddings = self.softmax(embeddings)\n",
    "#         return embeddings\n",
    "\n",
    "# class AudioTaggingModel(nn.Module):\n",
    "#     def __init__(self, ff_embedding_size, ff_output_size):\n",
    "#         super(AudioTaggingModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(ff_embedding_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, ff_output_size)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self, embeddings):\n",
    "#         embeddings = self.fc1(embeddings)\n",
    "#         embeddings = self.relu(embeddings)\n",
    "#         embeddings = self.fc2(embeddings)\n",
    "#         embeddings = self.softmax(embeddings)\n",
    "#         return embeddings\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, input_size=768, hidden_size=64, num_classes=188):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         self.fc = nn.Linear(465984, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc_output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "\n",
    "#         x = self.fc(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc_output(x)\n",
    "#         return x\n",
    "\n",
    "# # class CustomAudioModel(nn.Module):\n",
    "# #     def __init__(self, dataset, ff_input_size=embedding_size, ff_output_size=64, num_classes=188):\n",
    "# #         super(CustomAudioModel, self).__init__()\n",
    "        \n",
    "# #         self.dataset = dataset\n",
    "\n",
    "# #         # Define custom feed-forward layers\n",
    "# #         self.fc1 = nn.Linear(ff_input_size, ff_output_size)\n",
    "# #         self.relu = nn.ReLU()\n",
    "# #         self.fc2 = nn.Linear(ff_output_size, num_classes)\n",
    "\n",
    "# #     def forward(self, embeddings):\n",
    "        \n",
    "# #         embeddings = self.dataset['embeddings']  # Grab the audio embeddings from the inputs dictionary\n",
    "# #         # Flatten the embeddings\n",
    "# #         embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "# #         # Apply custom feed-forward layers directly to the flattened embeddings\n",
    "# #         x = self.fc1(embeddings)\n",
    "# #         x = self.relu(x)\n",
    "# #         x = self.fc2(x)\n",
    "\n",
    "# #         return x\n",
    "\n",
    "# #     from transformers import Trainer, TrainingArguments\n",
    "# # from torch.optim import Adam\n",
    "# # import torch.nn.functional as F\n",
    "# # #%pip install \"transformers[torch]\"\n",
    "\n",
    "# # Instantiate the model\n",
    "# #model = CustomAudioModel(ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "\n",
    "# # # #These need to be updated\n",
    "# # # csv_path_train = 'data.csv'\n",
    "# # # csv_path_val = 'data.csv'\n",
    "\n",
    "# # # #Creating train and validate datasets\n",
    "# # # train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "# # # val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# # # Loss function for multi-label classification\n",
    "# # # def compute_loss(model, inputs):\n",
    "# # #     # Your custom loss calculation goes here\n",
    "# # #     logits = model(inputs['input'])\n",
    "# # #     loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "# # #     return loss\n",
    "\n",
    "# # #Loss function for binary, multi-label classification\n",
    "\n",
    "# # #loss_fn = nn.BCEWithLogitsLoss() #appropiate loss function for multi-label classification where each label is binary. \n",
    "\n",
    "# # # Optimizer\n",
    "# # optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# # Training arguments -- these need to be adjusted\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir='./results',                     # output directory\n",
    "# #     num_train_epochs=3,                         # total number of training epochs\n",
    "# #     per_device_train_batch_size=32,             # batch size per device during training\n",
    "# #     per_device_eval_batch_size=32,              # batch size per device during eval\n",
    "# #     #weight_decay=0.01,                         # regularization parameter\n",
    "# #     logging_dir='./logs',                       # directory for storing logs\n",
    "# #     logging_steps=10,                           # number of steps before logging\n",
    "# #     evaluation_strategy=\"steps\",                # evaluate every eval_steps\n",
    "# #     eval_steps=50,                              # number of steps before evaluating\n",
    "# #     save_total_limit=2,                         # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "# #     save_steps=500,                             # number of updates steps before checkpoint saves                       \n",
    "# # )\n",
    "\n",
    "# # # Trainer instance\n",
    "# # trainer = Trainer(\n",
    "# #     model=model,\n",
    "# #     args=training_args,\n",
    "# #     train_dataset=train_example,\n",
    "# #     eval_dataset=val_example,\n",
    "# #     #compute_loss=loss_fn,\n",
    "# #     #optimizer=optimizer\n",
    "# # )\n",
    "\n",
    "# # # Train the model\n",
    "# # trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "\n",
    "# # Replace 'path_to_your_audio_file.wav' with the path to one of your audio files\n",
    "# #audio_path = 'train/aba_structure-epic-01-deep_step-291-320.wav'\n",
    "# audio_path = 'train/aba_structure-epic-01-deep_step-320-349.wav'\n",
    "\n",
    "# try:\n",
    "#     audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "#     print(\"Successfully loaded audio file.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading audio file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from torch.optim import Adam\n",
    "# import torch.nn.functional as F\n",
    "# #%pip install \"transformers[torch]\"\n",
    "\n",
    "# # Instantiate the model\n",
    "# # model = CustomAudioModel(dataset = train_example, ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "# model = AudioTaggingModel(465984, 188)\n",
    "# # #These need to be updated\n",
    "# # csv_path_train = 'data.csv'\n",
    "# # csv_path_val = 'data.csv'\n",
    "\n",
    "# # #Creating train and validate datasets\n",
    "# # train_dataset = CustomAudioDataset(csv_path_train, processor)\n",
    "# # val_dataset = CustomAudioDataset(csv_path_val, processor)\n",
    "\n",
    "# # Loss function for multi-label classification\n",
    "# # def compute_loss(model, inputs):\n",
    "# #     # Your custom loss calculation goes here\n",
    "# #     logits = model(inputs['input'])\n",
    "# #     loss = F.binary_cross_entropy_with_logits(logits, inputs['labels']) #appropiate loss function for multi-label classification\n",
    "# #     return loss\n",
    "\n",
    "# #Loss function for binary, multi-label classification\n",
    "# class CustomTrainer(Trainer):\n",
    "#     loss_fn = nn.BCEWithLogitsLoss() #appropiate loss function for multi-label classification where each label is binary. \n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = Adam(model.parameters(), lr=.001)\n",
    "\n",
    "# # Training arguments -- these need to be adjusted\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',                     # output directory\n",
    "#     num_train_epochs=3,                         # total number of training epochs\n",
    "#     per_device_train_batch_size=5,             # batch size per device during training\n",
    "#     per_device_eval_batch_size=5,              # batch size per device during eval\n",
    "#     #weight_decay=0.01,                         # regularization parameter\n",
    "#     logging_dir='./logs',                       # directory for storing logs\n",
    "#     logging_steps=10,                           # number of steps before logging\n",
    "#     evaluation_strategy=\"steps\",                # evaluate every eval_steps\n",
    "#     eval_steps=50,                              # number of steps before evaluating\n",
    "#     save_total_limit=2,                         # limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "#     save_steps=500,                             # number of updates steps before checkpoint saves                       \n",
    "# )\n",
    "\n",
    "# # Trainer instance\n",
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_example,\n",
    "#     eval_dataset=val_example,\n",
    "# #     compute_loss=loss_fn,\n",
    "#     #optimizer=optimizer\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model after training\n",
    "# model_path = \"./example_50\"\n",
    "# model.save_pretrained(model_path)\n",
    "# processor.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "# from torch.optim import Adam\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # input_size = 768  # Update with the actual size of the embeddings\n",
    "# # hidden_size = 64  # Adjust based on your architecture\n",
    "# # num_classes = 188  # Adjust based on the number of classes\n",
    "\n",
    "# # model = AudioTaggingModel(dataset = train_example, ff_input_size=embedding_size, ff_output_size=64, num_classes=188) \n",
    "\n",
    "# model = AudioTaggingModel(465984, 188)\n",
    "\n",
    "# # Set up your training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     save_total_limit=2,\n",
    "#     save_steps=500,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=100,\n",
    "#     learning_rate=2e-5,\n",
    "# )\n",
    "# # Instantiate the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_example,  # Assuming you have train_example defined\n",
    "#     eval_dataset=val_example,  # Assuming you have val_example defined\n",
    "#     data_collator=None,  # You can customize the data collator if needed\n",
    "#     compute_metrics=None, # You can define your own metrics function if needed\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model after training\n",
    "# model_path = \"./example_model\"\n",
    "# model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomAudioModel(nn.Module):\n",
    "#     def __init__(self, wav2vec_model_name=\"facebook/wav2vec2-base-960h\", output_size=188, ff_output_size=64):\n",
    "#         super(CustomAudioModel, self).__init__()\n",
    "\n",
    "#         # Load the Wav2Vec 2.0 model and processor\n",
    "#         self.wav2vec_model = AutoModelForCTC.from_pretrained(wav2vec_model_name)\n",
    "#         self.processor = AutoProcessor.from_pretrained(wav2vec_model_name)\n",
    "\n",
    "#         # Define custom feed-forward layers\n",
    "#         self.fc1 = nn.Linear(768, ff_output_size)  # Adjust input size based on Wav2Vec 2.0 model's hidden size\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(ff_output_size, output_size)  # Adjust output size based on your task\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, audio_data):\n",
    "#         # Process audio data using the Wav2Vec 2.0 model\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "#         with torch.no_grad():\n",
    "#             embeddings = self.wav2vec_model(input_tensors).last_hidden_state.mean(dim=1)\n",
    "\n",
    "#         # Apply custom feed-forward layers\n",
    "#         x = self.fc1(embeddings)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         output = self.softmax(x)  # Apply sigmoid activation for multi-label classification\n",
    "\n",
    "#         return output\n",
    "\n",
    "#Instantiate the model\n",
    "# model = CustomAudioModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# num_epochs = 3\n",
    "\n",
    "# audio_data = torch.randn(1, 16000)  # Replace with your actual audio data\n",
    "# output = model(audio_data)\n",
    "# print(output.shape)  # This will be (batch_size, output_size), where output_size is 188 in your case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         inputs = batch['input']\n",
    "#         labels = batch['labels']\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute the loss\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "# import torch\n",
    "# import librosa\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset from the CSV file\n",
    "# csv_path = 'path/to/your/csv/file.csv'\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Load pretrained model and processor\n",
    "# model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# class CustomAudioDataset(Dataset):\n",
    "#     def __init__(self, dataframe, processor):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         audio_path = self.dataframe.iloc[idx]['audio_file']\n",
    "#         labels = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "#         # Load audio file and process using the Wav2Vec processor\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=16000)\n",
    "#         input_tensors = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "#         return {'input': input_tensors, 'labels': torch.tensor(labels, dtype=torch.float32)}\n",
    "\n",
    "# # Create an instance of your custom dataset\n",
    "# audio_dataset = CustomAudioDataset(df, processor)\n",
    "\n",
    "# # Create a PyTorch DataLoader for batching and shuffling\n",
    "# batch_size = 32  # Adjust as needed\n",
    "# data_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example usage in a training loop\n",
    "# for batch in data_loader:\n",
    "#     inputs = batch['input']\n",
    "#     labels = batch['labels']\n",
    "\n",
    "    # Forward pass, loss calculation, backward pass, optimization, etc.\n",
    "    # Your training code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Path to the directory containing the audio files\n",
    "# train_audio = 'train'\n",
    "\n",
    "# #list all files in the directory\n",
    "# audio_files = [os.path.join(train_audio, file) for file in os.listdir(train_audio)]\n",
    "\n",
    "# #Define number of files to process\n",
    "# num_files = 10\n",
    "\n",
    "# #iterate over audio file and extract embeddings.\n",
    "\n",
    "# for i, audio_file in enumerate(audio_files):\n",
    "#   embeddings = extract_audio_embeddings(audio_file)\n",
    "  \n",
    "#   #save embeddings in a numpy array\n",
    "#   if i == 0:\n",
    "#     embeddings_array = embeddings\n",
    "#   else:\n",
    "#     embeddings_array = np.vstack((embeddings_array, embeddings))\n",
    "    \n",
    "#   #Check if number of files to process has been reached\n",
    "#   if i + 1 == num_files:\n",
    "#     print(f'Processed {num_files} files. Stopping the iteration.')\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
