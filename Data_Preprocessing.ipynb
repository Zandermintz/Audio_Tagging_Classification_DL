{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Necessary Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the baseline system, \n",
    "\n",
    "Output final ==> data, pre-processing, encoding, \n",
    "\n",
    "Scale down the audio so we can use less memory?\n",
    "\n",
    "pre-trained model? \n",
    "\n",
    "Training and fine-tuning the model (training 3-4 times, average results and report results),\n",
    "\n",
    "Output would be model \n",
    "\n",
    "print output and plot heatmaps in forward block (can we see more insights into how the model can capture genere?, Could be good to error analysis)\n",
    "\n",
    "Wave2vec - base encoder, add FF elements and softmax and fine-tune\n",
    "\n",
    "model.encoder.pre-trained \n",
    "\n",
    "start with audio for now\n",
    "\n",
    "create three folders randomly (85/5/10) - read one directory, sort file names, take next 85%/5%/15% only take file name, go inside csv \n",
    "\n",
    "Huggingface wav2vec 2.0 source code -> models within models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install librosa\n",
    "%pip install sklearn\n",
    "%pip install pydub\n",
    "%pip install soundfile\n",
    "\n",
    "#ffmpeg is a dependency for pydub. Can install at the system level with:\n",
    "# brew install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load appropriate Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility for PyTorch and NumPy.\n",
    "\n",
    "    Args:\n",
    "        seed_value (int): The seed value to set for random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "    # Additional steps for deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)  # You can replace 42 with any other seed value of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Management to work with Audio Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the zip files into single zip file\n",
    "%cat mp3.zip.* > single_mp3.zip\n",
    "\n",
    "# unzip the mp3 files\n",
    "%unzip mp3.zip\n",
    "\n",
    "#move all mp3 files into mp3 folder\n",
    "%find . -type f -name \"*.wav\" -exec mv {} ./mp3/ \\;\n",
    "\n",
    "#remove all zip files\n",
    "%rm -r 0 1 2 3 4 5 6 7 8 9 a b c d e f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Audio in MP3 to WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting mp3 files to wav files. This takes ~75 minutes to run locally.\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Directory where mp3 files are stored\n",
    "mp3_dir = './mp3'\n",
    "\n",
    "#Directory where wav files will be saved\n",
    "wav_dir = './wav'\n",
    "\n",
    "#Checking to see if wav directory exists\n",
    "os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "#counter to keep track of audio files\n",
    "processed = 0\n",
    "\n",
    "#Loop through all mp3 files in mp3 directory\n",
    "for mp3 in os.listdir(mp3_dir):\n",
    "    if processed >= 26000:\n",
    "        break #stop processing files after the number above has been processed\n",
    "    \n",
    "    if mp3.endswith('.mp3'):\n",
    "        mp3_path = os.path.join(mp3_dir, mp3)\n",
    "        try:\n",
    "            #Loading mp3 file\n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            \n",
    "            #Create the output wav file path by changing the extension from mp3 to wav\n",
    "            wav_file = mp3.replace('.mp3', '.wav')\n",
    "            wav_path = os.path.join(wav_dir, wav_file)\n",
    "\n",
    "            #Export audio as wav file\n",
    "            audio.export(wav_path, format='wav')\n",
    "            print(f'Converted: {mp3} -> {wav_file}')\n",
    "            \n",
    "            #Increment counter\n",
    "            processed += 1\n",
    "        except Exception as e:\n",
    "            print(f'Error converting: {mp3}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the distribution of the label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "annotations = pd.read_csv('./annotations_final.csv', sep='\\t')\n",
    "# clip_data = pd.read_csv('./clip_info_final.csv', sep= '\\t')\n",
    "\n",
    "#Check shape of data\n",
    "\n",
    "# !find ./wav/ -type f | wc -l \n",
    "#print(annotations.shape)\n",
    "# print(clip_data.shape)\n",
    "\n",
    "#for each column, count the number of 1s and store in a dictionary\n",
    "\n",
    "annotations_dict = {}\n",
    "for column in annotations.columns:\n",
    "    annotations_dict[column] = annotations[column].sum()\n",
    "    \n",
    "#remove first and last column    \n",
    "del annotations_dict['clip_id'], annotations_dict['mp3_path']\n",
    "\n",
    "#visualize distribution of variables in annotations\n",
    "plt.bar(annotations_dict.keys(), annotations_dict.values())\n",
    "\n",
    "#make the plot bigger\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "\n",
    "#rotate x axis labels\n",
    "plt.xticks(rotation=90)\n",
    "#make font size bigger\n",
    "plt.rcParams.update({'font.size': 21})\n",
    "#change font \n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the average number of labels per clip\n",
    "\n",
    "annotations_values = annotations_dict.values()\n",
    "means = sum(annotations_values)/len(annotations_values)\n",
    "\n",
    "above_average = []\n",
    "above_1000 = []\n",
    "above_2000 = []\n",
    "above_3000 = []\n",
    "above_4000 = []\n",
    "\n",
    "for key, value in annotations_dict.items():\n",
    "    if value > means:\n",
    "        above_average.append(key)\n",
    "    if value > 1000:\n",
    "        above_1000.append(key)\n",
    "    if value > 2000:\n",
    "        above_2000.append(key)\n",
    "    if value > 3000:\n",
    "        above_3000.append(key)\n",
    "    if value > 4000:\n",
    "        above_4000.append(key)\n",
    "\n",
    "print(f'Average number of labels per genre = {means.round()}')\n",
    "print(f'We have {len(above_1000)} Genres with more than 1000 labels. They are: {above_1000}')\n",
    "print(f'We have {len(above_2000)} Genres with more than 2000 labels. They are: {above_2000}')\n",
    "print(f'We have {len(above_3000)} Genres with more than 3000 labels. They are: {above_3000}')\n",
    "print(f'We have {len(above_4000)} Genres with more than 4000 labels. They are: {above_4000}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure paths for annotations and text metadata match with audio directory. Also that the file types are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_path(file):\n",
    "    '''\n",
    "    To clean the path of the file to merge the datasets\n",
    "    '''\n",
    "    file['mp3_path'] = file['mp3_path'] = file['mp3_path'].str.split('/').str[1] #replace the path with the correct path\n",
    "    file['mp3_path'] = file['mp3_path'].str.replace('.mp3', '.wav') #change the file type to .wav\n",
    "\n",
    "    #print the first few rows to check\n",
    "    print(file['mp3_path'].head(3))\n",
    "    \n",
    "    return file\n",
    "\n",
    "#clip_data_clean = clean_path(clip_data)\n",
    "annotations_clean = clean_path(annotations)\n",
    "\n",
    "#output the new clip_data_clean into csv to inspect\n",
    "\n",
    "# clip_data_clean.to_csv('clip_data_clean.csv', index=False)\n",
    "# annotations_clean.to_csv('annotations_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate separate folders for test, train, validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three separate folders from the .wav files. One for train (85% of files), one for validation (5%) of files, and one for test (10%) of files.\n",
    "\n",
    "# create a list of all the files in the wav directory\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "#set the path to the wav files\n",
    "wav_dir = './wav'\n",
    "\n",
    "#set the path to the train, validation, and test folders\n",
    "train_dir = './train'\n",
    "valid_dir = './valid'\n",
    "test_dir = './test'\n",
    "\n",
    "#check if the folders exist, if not create them\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#Get a sorted list of all the files in the wav directory\n",
    "all_files = sorted(os.listdir(wav_dir))\n",
    "\n",
    "#set the number of files to put into train folder\n",
    "train_size = int(len(os.listdir(wav_dir)) * 0.85)\n",
    "\n",
    "#set the number of files to put into valid folder\n",
    "valid_size = int(len(os.listdir(wav_dir)) * 0.05)\n",
    "\n",
    "#set the number of files to put into test folder\n",
    "test_size = int(len(os.listdir(wav_dir)) * 0.1)\n",
    "\n",
    "#Create list of first 85% of files to put into train folder\n",
    "train_files = all_files[:train_size]\n",
    "\n",
    "#Create list of the next 5% of files to put into valid folder\n",
    "valid_files = all_files[train_size:train_size + valid_size]\n",
    "\n",
    "#Create list of the next 10% of files to put into test folder\n",
    "test_files = all_files[train_size + valid_size:]\n",
    "\n",
    "#check the length of each list to make sure they add up to the total number of files\n",
    "print(f'Total number of files: {len(all_files)}')\n",
    "print(f'Number of files in train_list: {len(train_files)}')\n",
    "print(f'Number of files in valid_list: {len(valid_files)}')\n",
    "print(f'Number of files in test_list: {len(test_files)}')\n",
    "\n",
    "#move the files into the correct folders\n",
    "\n",
    "#move the train files\n",
    "for file in train_files:\n",
    "    shutil.move(os.path.join(wav_dir, file), train_dir)\n",
    "    \n",
    "#move the valid files\n",
    "for file in valid_files:\n",
    "    shutil.move(os.path.join(wav_dir, file), valid_dir)\n",
    "    \n",
    "#move the test files\n",
    "for file in test_files:\n",
    "    shutil.move(os.path.join(wav_dir, file), test_dir)\n",
    "    \n",
    "#check the number of files in each folder\n",
    "train_files_in_folder = os.listdir(train_dir)\n",
    "valid_files_in_folder = os.listdir(valid_dir)\n",
    "test_files_in_folder = os.listdir(test_dir)\n",
    "    \n",
    "#check the number of files in each folder\n",
    "print(f'Number of files in train_folder: {len(train_files_in_folder)}')\n",
    "print(f'Number of files in valid_folder: {len(valid_files_in_folder)}')\n",
    "print(f'Number of files in test_folder: {len(test_files_in_folder)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort annotations file by mp3_path\n",
    "\n",
    "annotations_clean = annotations_clean.sort_values(by=['mp3_path'])\n",
    "\n",
    "#Function to use the files in each test, train, valid folder to split the annotations dataframe into three dataframes corresponding to each folder.\n",
    "\n",
    "def test_train_valid (dir, annotations):\n",
    "    x = ''\n",
    "    if dir.startswith('train_dir'):\n",
    "        x = 'train'\n",
    "    elif dir.startswith('valid_dir'):\n",
    "        x = 'valid'\n",
    "    elif dir.startswith('test_dir'):\n",
    "        x = 'test'\n",
    "    \n",
    "    file_list = os.listdir(dir)\n",
    "    file_list = [file for file in file_list]\n",
    "    \n",
    "    #split the annotations dataframe based on the file names in the list\n",
    "    subset_df = annotations[annotations['mp3_path'].isin(file_list)]\n",
    "    \n",
    "    return x, subset_df\n",
    "\n",
    "# run the function on each folder\n",
    "train_y, train_df = test_train_valid(train_dir, annotations_clean)\n",
    "valid_y, valid_df= test_train_valid(valid_dir, annotations_clean)\n",
    "test_y, test_df = test_train_valid(test_dir, annotations_clean)\n",
    "\n",
    "#check the shape of each dataframe\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Valid shape: {valid_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to see if all files in test, train, validate folders match those in the corresponding dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(train_files_in_folder) == set(train_df['mp3_path']):\n",
    "    print(\"The file names in the train folder match the file names in the train dataframe\")\n",
    "else:\n",
    "    print(\"The file names in the train folder do not match the file names in the train dataframe\")\n",
    "    \n",
    "if set(valid_files_in_folder) == set(valid_df['mp3_path']):\n",
    "    print(\"The file names in the valid folder match the file names in the valid dataframe\")\n",
    "else:\n",
    "    print(\"The file names in the valid folder do not match the file names in the valid dataframe\")\n",
    "    \n",
    "if set(test_files_in_folder) == set(test_df['mp3_path']):\n",
    "    print(\"The file names in the test folder match the file names in the test dataframe\")\n",
    "else:\n",
    "    print(\"The file names in the test folder do not match the file names in the test dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# load pretrained model and processor\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def extract_audio_embeddings(audio_file_name):\n",
    "  audio_data, _= torchaudio.load(audio_file_name, sample_rate = 16000) #how many samples are processed per second.\n",
    "  input_tensors = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000).input_values #use processor to process audio file and return tensor of input values for model\n",
    "  with torch.no_grad():\n",
    "    embeddings = model(input_tensors).last_hidden_state.mean(dim=1) #take the mean of the last hidden state of the model to extract the embeddings\n",
    "  print (embeddings.shape)\n",
    "  return embeddings.flatten()\n",
    "\n",
    "e1 = extract_audio_embeddings(\"train/aba_structure-epic-01-deep_step-59-88.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForCTC, AutoProcessor\n",
    "\n",
    "# Load pretrained model and processor\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def extract_audio_embeddings(audio_file_name):\n",
    "    try:\n",
    "        # Set the sampling rate\n",
    "        torchaudio.set_audio_backend(\"sox_io\")\n",
    "        \n",
    "        # Load audio data\n",
    "        audio_data, _ = torchaudio.load(audio_file_name, normalize=True)\n",
    "        \n",
    "        # Process audio data\n",
    "        input_tensors = processor(audio_data[0].numpy(), return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        \n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(input_tensors).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file {audio_file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "e1 = extract_audio_embeddings(\"train/aba_structure-epic-01-deep_step-59-88.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge annotations and text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Merge the metadata and annotations file on mp3_path variable\n",
    "# merge = pd.merge(clip_data_clean, annotations_clean, on='mp3_path', how='inner')\n",
    "\n",
    "# # keep everything except: url, original_url, clip_id_y \n",
    "# merge.drop(['url', 'original_url', 'clip_id_y'], axis=1, inplace=True)\n",
    "\n",
    "# #rename clip_id_x to clip_id\n",
    "# merge.rename(columns={'clip_id_x' : 'clip_id'}, inplace=True)\n",
    "\n",
    "# print(merge.shape)\n",
    "# print(merge.head(5))\n",
    "# #merge.to_csv('merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, validate, test, split. (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #split train data into train and validation sets. Use stratified sampling to ensure equal class distribution in train and validation sets: https://www.geeksforgeeks.org/how-to-split-a-dataset-into-train-and-test-sets-using-python/\n",
    "\n",
    "# #split merge into train, test and validate sets\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(merge['text'], train['label'], test_size=0.05, random_state=42, stratify=train['label'])\n",
    "\n",
    "\n",
    "# #check the shape of train and validation sets\n",
    "# # print(f'----Shape_Check----')\n",
    "# # print('\\n')\n",
    "# # print(f'The length of the original train and test set are: {len(train), len(test)}')\n",
    "# # print(f'The length of text (x) for training and validation are: {x_train.shape, x_val.shape}.')\n",
    "# # print(f'The length of labels (y) for training and validation are: {y_train.shape, y_val.shape}.')\n",
    "# # print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Audio Features (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa as lr\n",
    "# import os\n",
    "\n",
    "# #function to extract features from audio file\n",
    "\n",
    "# audio_dir = './test'\n",
    "\n",
    "# def extract_audio_features(audio_dir):\n",
    "#     for filename in os.listdir(audio_dir):\n",
    "#         if filename.endswith('.wav'):\n",
    "#             audio_path = os.path.join(audio_dir, filename)\n",
    "#             #load audio file\n",
    "#             y, sr = lr.load(audio_path, sr = 22050) #y = audio signal over time, sr = sampling rate, or how manu audio samples are recorded per second. Default sampling rate of 22050 Hz.\n",
    "#             #extract features\n",
    "#             mfccs = lr.feature.mfcc(y=y, sr=sr) #mfcss = variable that stores the Mel-frequency cepstral coefficients (MFCCs) extracted from the audio. These capture the timbral/textural aspects of the audio signal (e.g. brightness, nasality, roughness, etc.). Default number of MFCCs to return is 20.\n",
    "#             yield mfccs, mfccs #yield is a keyword that is used like return, except the function will return a generator. Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly.\n",
    "\n",
    "# #Process audio files using the generator\n",
    "# for filename, mfccs in extract_audio_features(audio_dir):\n",
    "#     print(f\"Processed: {len(filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing Approach to converting mp3 -> wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pydub import AudioSegment\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# mp3_dir = './mp3'\n",
    "# wav_dir = './wav'\n",
    "# os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "# def convert_mp3_to_wav(mp3):\n",
    "#     try:\n",
    "#         mp3_path = os.path.join(mp3_dir, mp3)\n",
    "#         audio = AudioSegment.from_mp3(mp3_path)\n",
    "#         wav_file = mp3.replace('.mp3', '.wav')\n",
    "#         wav_path = os.path.join(wav_dir, wav_file)\n",
    "#         audio.export(wav_path, format='wav')\n",
    "#         print(f'Converted: {mp3} -> {wav_file}')\n",
    "#     except Exception as e:\n",
    "#         print(f'Error converting: {mp3}: {str(e)}')\n",
    "\n",
    "# processed = 0\n",
    "# max_files = 26000\n",
    "\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     futures = {executor.submit(convert_mp3_to_wav, mp3): mp3 for mp3 in os.listdir(mp3_dir) if mp3.endswith('.mp3')}\n",
    "\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         mp3 = futures[future]\n",
    "#         try:\n",
    "#             future.result()\n",
    "#             processed += 1\n",
    "#         except Exception as e:\n",
    "#             print(f'Error converting: {mp3}: {str(e)}')\n",
    "\n",
    "#         if processed >= max_files:\n",
    "#             break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
